# Model Configuration
model:
  block_size: 256      # Sequence length
  d_model: 512         # Embedding dimension
  num_heads: 32        # Number of attention heads
  num_layers: 4        # Number of transformer layers
  d_ff: 1024           # Feed-forward layer dimension
  dropout: 0.1         # Dropout rate
  memory_depth: 2 

# Training Configuration
training:
  seed: 42                # Random seed for reproducibility
  batch_size: 48          # Training batch size
  learning_rate: 0.00003  # Learning rate
  max_epochs: 10          # Maximum number of training epochs
  eval_interval: 1300     # Evaluation interval in steps
  grad_clip: 1.0          # Gradient clipping threshold
  dataset: "wikitext-103"

# Generation Configuration
generation:
  eval_examples: 1        # Number of examples to generate during evaluation
  max_new_tokens: 100     # Maximum number of tokens to generate
  temperature: 0.8        # Sampling temperature
  top_k: 40               # Top-k sampling parameter